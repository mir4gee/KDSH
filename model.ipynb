{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can use other BERT variants like 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # For binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(15, 384)\n",
      "15\n",
      "Vectorstore made\n"
     ]
    }
   ],
   "source": [
    "import data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=data_file.data1\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X=[]\n",
    "data_Y=[]\n",
    "for element in data_list:\n",
    "    data_X.append(element['content'])\n",
    "    data_Y.append(element['label'])\n",
    "data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(text, max_length=512, overlap=256):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    windows = []\n",
    "    for i in range(0, len(tokens), max_length - overlap):\n",
    "        window = tokens[i:i + max_length]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "tokenized_cont={}\n",
    "for i in range(len(data_X)):\n",
    "    windows = sliding_window_tokenize(data_X[i], max_length=512, overlap=256)\n",
    "    tokenized_cont[i]=windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum=0\n",
    "for i in range(len(tokenized_cont)):\n",
    "    sum+=len(tokenized_cont[i])\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Define the Dataset class\n",
    "class BinaryClassificationDataset(Dataset):\n",
    "    def __init__(self, tokenized_windows, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - tokenized_windows: Pre-tokenized windows of text (List[List[int]]).\n",
    "        - labels: Corresponding labels for each window (List[int]).\n",
    "        \"\"\"\n",
    "        self.tokenized_windows = tokenized_windows\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single data sample.\n",
    "        Args:\n",
    "        - idx: Index of the data sample.\n",
    "        Returns:\n",
    "        - A dictionary with 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenized_windows[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Generate attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(input_ids) + [0] * (512 - len(input_ids))\n",
    "\n",
    "        # Pad input_ids to max_length (512)\n",
    "        padded_input_ids = input_ids + [0] * (512 - len(input_ids))\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare tokenized windows and labels\n",
    "all_windows = []\n",
    "all_labels = []\n",
    "\n",
    "# Dynamically populate windows and replicate labels\n",
    "for idx, windows in tokenized_cont.items():\n",
    "    all_windows.extend(windows)  # Add all windows for the sample\n",
    "    all_labels.extend([data_Y[idx]] * len(windows))  # Replicate the label for each window\n",
    "\n",
    "# Verify size consistency\n",
    "assert len(all_windows) == len(all_labels), \"Mismatch between windows and labels!\"\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = BinaryClassificationDataset(all_windows, all_labels)\n",
    "\n",
    "# Check dataset size and a sample\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Sample data: {train_dataset[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.BinaryClassificationDataset"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import accelerate\n",
    "import torch\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # output directory for model predictions and checkpoints\n",
    "    num_train_epochs=1,                # number of training epochs\n",
    "    per_device_train_batch_size=1,     # batch size for training\n",
    "    per_device_eval_batch_size=1,      # batch size for evaluation\n",
    "    warmup_steps=500,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    logging_dir='./logs',              # directory for storing logs\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "# Setup Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,         # Pass the dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a validation dataset, pass it here\n",
    "trainer.evaluate(train_dataset)  # Pass your validation dataset (similar to train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./binary_classification_model')\n",
    "tokenizer.save_pretrained('./binary_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_windows_for_model(windows, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Converts tokenized windows into model-compatible tensors.\n",
    "    \n",
    "    Args:\n",
    "    - windows: List of tokenized windows.\n",
    "    - tokenizer: Pretrained tokenizer object.\n",
    "    - max_length: int, maximum sequence length for the model.\n",
    "    \n",
    "    Returns:\n",
    "    - input_tensors: List of dictionaries with 'input_ids' and 'attention_mask'.\n",
    "    \"\"\"\n",
    "    input_tensors = []\n",
    "    for window in windows:\n",
    "        # Decode tokens back into text for the tokenizer\n",
    "        text_window = tokenizer.decode(window, skip_special_tokens=True)\n",
    "        \n",
    "        # Tokenize and encode for the model\n",
    "        encoding = tokenizer(\n",
    "            text_window,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_tensors.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove extra batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        })\n",
    "    return input_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3057 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "input_text=\"\"\n",
    "with open('./Papers/P099.pdf', \"rb\") as f:\n",
    "    reader=PdfReader(f)\n",
    "    for pages in reader.pages:\n",
    "        input_text+=pages.extract_text()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./binary_classification_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./binary_classification_model')\n",
    "# Example inference\n",
    "prompt=input_text\n",
    "tokenized_text=prepare_windows_for_model(sliding_window_tokenize(prompt),tokenizer)\n",
    "output = model(input_ids=tokenized_text[0]['input_ids'].unsqueeze(0),attention_mask=tokenized_text[0]['attention_mask'].unsqueeze(0))\n",
    "\n",
    "# Convert logits to probabilities (if needed)\n",
    "logits = output.logits\n",
    "probabilities = torch.nn.Softmax(dim=1)(logits)\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_content(destination):\n",
    "    rows = []\n",
    "    for file_name in os.listdir(destination):\n",
    "        file_path = os.path.join(destination, file_name)\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            content=\"\"\n",
    "            for page in reader.pages:\n",
    "                content+=page.extract_text()            \n",
    "            rows.append({\"file_name\": file_name, \"file_content\": content})\n",
    "            print(file_name) \n",
    "    print(len(rows))  \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=load_content('./Papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('./binary_classification_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./binary_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'124'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[34]['file_name'][1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=np.zeros((135,2))\n",
    "for i in range(len(rows)):\n",
    "    tokenized_text=prepare_windows_for_model(sliding_window_tokenize(rows[i]['file_content']),tokenizer)\n",
    "    output = model(input_ids=tokenized_text[0]['input_ids'].unsqueeze(0),attention_mask=tokenized_text[0]['attention_mask'].unsqueeze(0))\n",
    "    logits = output.logits\n",
    "    probabilities = torch.nn.Softmax(dim=1)(logits)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    file_number=rows[i]['file_name'][1:4]\n",
    "    outputs[int(file_number)-1,0]=predicted_class\n",
    "    outputs[int(file_number)-1,1]=int(file_number)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=outputs[:,0].astype(int)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df=pd.DataFrame(labels)\n",
    "labels_df.to_csv('output.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
